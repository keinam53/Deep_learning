{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1_NLP_preprocessing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOB8qXtAI/3pt/PWy3vrZ59",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/keinam53/Deep_learning/blob/main/6_Rekurencyjne_Sieci_Neuronowe/1_NLP_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVAevez4980e"
      },
      "source": [
        "### Preprocessing danych tekstowch - wektoryzacja tekstu\n",
        "\n",
        "1. [Podział tekstu na słowa](#a0)\n",
        "2. [Kodowanie *one_hot()*](#a1)\n",
        "3. [Kodowanie *hashing_trick()*](#a2)\n",
        "4. [Tokenizer](#a3)\n",
        "\n",
        "Nie możemy wprowadzić bezpośrednio danych tekstowych do sieci neuronowej! Musimy te dane odpowiednio przygotować (preprocessing). Dane tekstowe muszą zostać zakodowane za pomocą liczb aby mogły być wprowadzone do sieci neuronowej. \n",
        "\n",
        "Biblioteka Keras zawiera kilka narzędzi, które możemy wykorzystać do przygotowania naszych danych."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTAXxnM69-Tc"
      },
      "source": [
        "### <a name='a0'></a> Podział tekstu na słowa\n",
        "\n",
        "Często w NLP - Natural Language Processing używamy słowa token. Tokenem może być pojedyńczy znak, a także cały wyraz. Wsystko zależy od kontekstu i potrzeb. Aby podzielić tekst na tokeny (w tym przypadku wyrazy) użyjemy funkcji *text_to_word_sequence()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkgl9EDL4T0X",
        "outputId": "3153a12d-c769-4c8e-8fc3-c7b7c876eb57"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = 'Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.'\n",
        "\n",
        "tokens = text_to_word_sequence(text)\n",
        "tokens"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['keras',\n",
              " 'is',\n",
              " 'a',\n",
              " 'high',\n",
              " 'level',\n",
              " 'neural',\n",
              " 'networks',\n",
              " 'api',\n",
              " 'written',\n",
              " 'in',\n",
              " 'python',\n",
              " 'and',\n",
              " 'capable',\n",
              " 'of',\n",
              " 'running',\n",
              " 'on',\n",
              " 'top',\n",
              " 'of',\n",
              " 'tensorflow',\n",
              " 'cntk',\n",
              " 'or',\n",
              " 'theano']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzyHeRCD-LKm"
      },
      "source": [
        "### <a name='a1'></a> Kodowanie *one_hot()*\n",
        "\n",
        "Nazwa sugeruje, że tworzymy kodowanie zero-jednykowe dokumentu, co nie jest prawdą. Polega na przedstawieniu każdego słowa jako unikalnej liczby całkowitej. *one_hot(text, n)* koduje tekst do listy indeksów słów o rozmiarze n. Jest to opakowanie funkcji hashing_trick używającej hash jako funkcji hashującej. \n",
        "\n",
        "Jednoznaczność mapowania słów na indeksy nie jest gwarantowana.  Zastosowanie funkcji hashującej może powodować kolizje i nie wszystkim słowom zostaną przypisane unikalne wartości całkowite.\n",
        "\n",
        "Oprócz tekstu należy podać rozmiar słownika. Może to być łączna liczba słów w dokumencie lub więcej, jeśli zamierzamy zakodować dodatkowe dokumenty zawierające dodatkowe słowa.\n",
        "\n",
        " Rozmiar słownika określa przestrzeń hashująca, z której słowa są hashowane. Najlepiej byłoby, gdyby był on większy niż słownik o pewien procent (np. 25%), aby zminimalizować liczbę kolizji. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0sPlFylEZXa",
        "outputId": "e7430e56-b142-497d-d28c-501231354007"
      },
      "source": [
        "hash('Mariusz')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4554421028388447353"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2vbB5-4TFFcq",
        "outputId": "48b31fef-9044-46a8-f7c5-5723cc79a9be"
      },
      "source": [
        "hash('Mariusz') % 100"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGKXg5vdFRSu"
      },
      "source": [
        "#set - usuwa powtarzające się elementy z listy\n",
        "set(tokens)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KhAug0xF1bK",
        "outputId": "80d82c34-6e2b-4c7c-d2ba-db59cfdddecb"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "words = set(tokens)\n",
        "one_hot_tokens = one_hot(text, round(len(words) * 1.3))\n",
        "one_hot_tokens"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[2,\n",
              " 26,\n",
              " 23,\n",
              " 12,\n",
              " 12,\n",
              " 20,\n",
              " 20,\n",
              " 26,\n",
              " 19,\n",
              " 15,\n",
              " 1,\n",
              " 21,\n",
              " 23,\n",
              " 16,\n",
              " 19,\n",
              " 9,\n",
              " 21,\n",
              " 16,\n",
              " 5,\n",
              " 22,\n",
              " 5,\n",
              " 25]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1qMyuPzI-NV"
      },
      "source": [
        "### <a name='a2'></a> Kodowanie *hashing_trick()*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHYdUMhNIHkw",
        "outputId": "9c438b1f-ee09-4dc2-9b21-4f74e59ded43"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import hashing_trick\n",
        "\n",
        "hashing_trick_tokens = hashing_trick(text, round(len(words) * 1.3), hash_function='md5')\n",
        "hashing_trick_tokens"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7,\n",
              " 20,\n",
              " 22,\n",
              " 24,\n",
              " 20,\n",
              " 14,\n",
              " 19,\n",
              " 11,\n",
              " 10,\n",
              " 4,\n",
              " 4,\n",
              " 15,\n",
              " 19,\n",
              " 15,\n",
              " 18,\n",
              " 23,\n",
              " 14,\n",
              " 15,\n",
              " 5,\n",
              " 20,\n",
              " 2,\n",
              " 8]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jydMOLQKD3M"
      },
      "source": [
        "### <a name='a3'></a> Tokenizer\n",
        "Keras dostarcza klasę Tokenizer do przygotowywania dokumentów tekstowych do uczenia głębokiego. \n",
        "\n",
        "Klasa *Tokenizer* pozwala zamienić każdy tekst na sekwencję liczb całkowitych w taki sposób, że każda liczba całkowita jest indeksem tokenu w słowniku. Tokenem zazwyczaj jest pojedyncze słowo. Zero jest zarezerwowanym ideksem i nie może zostać przypisane do żadnego słowa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHrUuNILJuKF",
        "outputId": "6a53338c-9e8f-40e8-a47b-c4446e12190d"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "\n",
        "samples = ['Great picture!', 'Nice view', 'Good to see you :)', 'Good picture!', 'Good']\n",
        "\n",
        "tokenizer.fit_on_texts(samples)\n",
        "\n",
        "tokenizer.index_word"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'good',\n",
              " 2: 'picture',\n",
              " 3: 'great',\n",
              " 4: 'nice',\n",
              " 5: 'view',\n",
              " 6: 'to',\n",
              " 7: 'see',\n",
              " 8: 'you'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVZLnglzUm0F",
        "outputId": "d7ec3aa1-346e-4178-a6d5-222137f09ecc"
      },
      "source": [
        "tokenizer.word_counts"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('great', 1),\n",
              "             ('picture', 2),\n",
              "             ('nice', 1),\n",
              "             ('view', 1),\n",
              "             ('good', 3),\n",
              "             ('to', 1),\n",
              "             ('see', 1),\n",
              "             ('you', 1)])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-vvuYCrUqgy",
        "outputId": "a859d30a-a095-4cf2-cf2d-806a83948156"
      },
      "source": [
        "tokenizer.document_count"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6YTKUDYVB8M"
      },
      "source": [
        "Po dopasowaniu Tokenizera do danych treningowych można go użyć do kodowania dokumentów danych treningowych jak i danych testowych.\n",
        "\n",
        "Funkcja *texts_to_matrix()* tworzy wektor dla każdego dokumentu. Długość wektora jest równa długości unikalnych słów we wszystkich dokumentach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XmvcOickU_FB",
        "outputId": "94f07e98-410a-4a47-aff3-99f5e77fea1f"
      },
      "source": [
        "print(tokenizer.index_word)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'good', 2: 'picture', 3: 'great', 4: 'nice', 5: 'view', 6: 'to', 7: 'see', 8: 'you'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUkwO3z5VLIn",
        "outputId": "65c882fc-8d0a-42c3-ea0f-70ae5c6a9bc1"
      },
      "source": [
        "tokenizer.texts_to_matrix(samples)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 1., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 1., 1., 1.],\n",
              "       [0., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    }
  ]
}